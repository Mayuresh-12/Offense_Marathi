{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the local git repo for Indic NLP library\n",
    "INDIC_NLP_LIB_HOME=r\"/Users/mayureshnene/Desktop/indic_nlp_library\"\n",
    "\n",
    "# The path to the local git repo for Indic NLP Resources\n",
    "INDIC_NLP_RESOURCES=r\"/Users/mayureshnene/Desktop/indic_nlp_resources\"\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from indicnlp import common\n",
    "from indicnlp import loader\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from indicnlp.tokenize import indic_tokenize \n",
    "from indicnlp.morph import unsupervised_morph \n",
    "from indicnlp import common\n",
    "\n",
    "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
    "\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
    "\n",
    "loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        NULL\n",
       "1        NULL\n",
       "2        NULL\n",
       "3        NULL\n",
       "4        NULL\n",
       "         ... \n",
       "12990    NULL\n",
       "12991    NULL\n",
       "12992    NULL\n",
       "12993    NULL\n",
       "12994    NULL\n",
       "Name: subtask_c, Length: 12995, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = pd.read_csv(\"/Users/mayureshnene/Desktop/Mayuresh/Offense_Marathi/Data/Data.csv\")\n",
    "training_dataset.dropna()\n",
    "training_dataset['subtask_c'].fillna(\"NULL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        आजच्या जनता दरबारात जळगाव जिल्ह्यातील चाळीसगाव...\n",
       "1        कुणी कविता करत असतं तर कुणी कविता जगत असतं कुण...\n",
       "2        आम्हाला इतिहासातील औरंगजेबशी काही घेणे नाही आम...\n",
       "3        गँभीर प्रकरण महाराष्ट्राची अवस्था बिकट आहे भाष...\n",
       "4        कब्झा हा कन्नड चित्रपट लवकरच मराठी मध्ये डब्ब ...\n",
       "                               ...                        \n",
       "12990    पहिल्या लाटे नंतर सर्वात कमी रुग्ण\\n#कोरोना #र...\n",
       "12991    RT @DeshRat30997789: @RavindraAmbekar जगातले स...\n",
       "12992    Tata Tigor EV | देशातील सर्वात स्वस्त इलेक्ट्र...\n",
       "12993    RT @PIBMumbai: पंतप्रधान आयुष्मान भारत आरोग्यव...\n",
       "12994    @Hopeful_Addict काटकसर म्हणजे गोधड्या शिवणे.\\n...\n",
       "Name: tweet, Length: 12995, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = training_dataset[\"tweet\"]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_A = training_dataset[[\"subtask_a\"]]\n",
    "level_B = training_dataset.query(\"subtask_a == 'Offensive'\")[[\"subtask_b\"]]\n",
    "level_C = training_dataset.query(\"subtask_b == 'TIN'\")[[\"subtask_c\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['अधिक ', 'अनेक ', 'अशी ', 'असलयाचे', 'असलेल्या', 'असा', 'असून', 'असे', 'आज', 'आणि', 'आता', 'आपल्या', 'आला', 'आली', 'आले', 'आहे', 'आहेत', 'एक', 'एका', 'कमी', 'करणयात', 'करून', 'का', 'काम', 'काय', 'काही', 'किवा', 'की', 'केला', 'केली', 'केले', 'कोटी', 'गेल्या', 'घेऊन', 'जात', 'झाला', 'झाली', 'झाले', 'झालेल्या', 'टा', 'डॉ', 'तर', 'तरी', 'तसेच', 'ता', 'ती', 'तीन', 'ते', 'तो', 'त्या', 'त्याचा', 'त्याची', 'त्याच्या', 'त्याना', 'त्यानी', 'त्यामुळे', 'त्री', 'दिली', 'दोन', 'न', 'नाही', 'निर्ण्य', 'पण', 'पम', 'परयतन', 'पाटील', 'म', 'मात्र', 'माहिती', 'मी', 'मुबी', 'म्हणजे', 'म्हणाले', 'म्हणून', 'या', 'याचा', 'याची', 'याच्या', 'याना', 'यानी', 'येणार', 'येत', 'येथील', 'येथे', 'लाख', 'व', 'व्यकत', 'सर्व', 'सागित्ले', 'सुरू', 'हजार', 'हा', 'ही', 'हे', 'होणार', 'होत', 'होता', 'होती', 'होते']\n"
     ]
    }
   ],
   "source": [
    "stopwords_file = open(\"/Users/mayureshnene/Desktop/MOLD/Mold/stopwords.txt\")\n",
    "stopwords = stopwords_file.read().splitlines()\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(row):\n",
    "    row = str(row)\n",
    "#     removal_list = ['URL','\\'ve','n\\'t','\\'s','\\'m','!']\n",
    "#     for element in removal_list:\n",
    "#         row = row.replace(element,'')\n",
    "    \n",
    "    row = row.replace('http\\S+|www.\\S+', '')\n",
    "    row = re.sub(\"@[A-Za-z0-9]+\",\"@USER\",row)\n",
    "    row = re.sub(\"[A-Za-z0-9]+\",\"\",row)\n",
    "    row = re.sub(\"@\",\"@USER\",row)\n",
    "    row = re.sub('[+,-,_,=,/,<,>,!,#,$,%,^,&,*,\\\",:,;,.,' ',\\t,\\r,\\n,\\',|]','',row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tweet in tweets:\n",
    "#     tweet = remove_noise(str(tweet))\n",
    "iterator_map = map(clean,tweets)\n",
    "tweets = list(iterator_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "collective_tweets = copy.deepcopy(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "analyzer=unsupervised_morph.UnsupervisedMorphAnalyzer('mr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    return indic_tokenize.trivial_tokenize(tweet)\n",
    "\n",
    "def morph(tweet):\n",
    "    analyzed_tokens=analyzer.morph_analyze_document(str(tweet).split(' '))\n",
    "    return analyzed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenize..: 100%|██████████| 12995/12995 [00:02<00:00, 4762.07it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"Tokenize..\")\n",
    "#all_tweets[\"tokens\"] = all_tweets['tweet'].progress_apply(tokenize)\n",
    "collective_tweets[\"tokens\"] = collective_tweets['tweet'].progress_apply(morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = collective_tweets[\"tokens\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfid_vectorizer(vector):\n",
    "\t## Creates and stores an instance of the TfidfVectorizer class. This will be used further to extract our data as tf-idf features.\n",
    "\tvectorizer = TfidfVectorizer()\n",
    "\tuntokenized_data =[' '.join(tweet) for tweet in tqdm(vector, \"Vectorizing...\")]\n",
    "\tvectorizer = vectorizer.fit(untokenized_data)\n",
    "\tvectors = vectorizer.transform(untokenized_data).toarray()\n",
    "\treturn vectors\n",
    "\n",
    "def get_vectors(vectors, labels, keyword):\n",
    "\t'''\n",
    "\tReturns a matrix for vectors. Zips vectors and labels IF and only if length of vector list is the same as length of the labels list. \n",
    "\tElse, the function gets terminated.\n",
    "\t@param vectors These are the vectors for a given label.\n",
    "\t@param labels These are the label values for the given label.\n",
    "\t@param keyword which is the label to annotate for.\n",
    "\t'''\n",
    "\tif len(vectors) != len(labels):\n",
    "\t\tprint(\"Unmatching sizes!\")\n",
    "\t\treturn\n",
    "\t\n",
    "\t## Stores a new list to append the zipped vectors and labels into.\n",
    "\tresult = list()\n",
    "\tfor vector, label in zip(vectors, labels):\n",
    "\t\tif label == keyword:\n",
    "\t\t\tresult.append(vector)\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing...: 100%|██████████| 12995/12995 [00:00<00:00, 793677.09it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors_level_A = tfid_vectorizer(vector)\n",
    "labels_level_a = level_A['subtask_a'].values.tolist()\n",
    "vectors_level_B = get_vectors(vectors_level_A, labels_level_a, \"Offensive\") \n",
    "\n",
    "labels_level_b = level_B['subtask_b'].values.tolist() \n",
    "\n",
    "## Numerical Vectors C\n",
    "vectors_level_c = get_vectors(vectors_level_B, labels_level_b, \"TIN\") \n",
    "\n",
    "##Subtask C Labels\n",
    "labels_level_c = level_C['subtask_c'].values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begins on Level A classification...\n"
     ]
    }
   ],
   "source": [
    "train_vectors_level_A, train_labels_level_A,= vectors_level_A[1:], labels_level_a[1:]\n",
    "test_vectors_level_A, test_labels_level_A = vectors_level_A[3097:], labels_level_a[3097:]\n",
    "## Extracting names of labels and storing them in a variable\n",
    "classNames = np.unique(test_labels_level_A)\n",
    "\n",
    "print(\"Training begins on Level A classification...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "## Creating an object of SVC\n",
    "classifiersvc = SVC()\n",
    "classifiermnb = MultinomialNB()\n",
    "classifiersgd = SGDClassifier()\n",
    "classifiermlp = MLPClassifier()\n",
    "\n",
    "classifiersvc.fit(train_vectors_level_A, train_labels_level_A)\n",
    "classifiermnb.fit(train_vectors_level_A, train_labels_level_A)\n",
    "classifiersgd.fit(train_vectors_level_A, train_labels_level_A)\n",
    "classifiermlp.fit(train_vectors_level_A, train_labels_level_A)\n",
    "\n",
    "# accuracy = accuracy_score(train_labels_level_A, classifiersvc.predict(train_vectors_level_A))\n",
    "# print(\"Training Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "test_predictions_A_svc = classifiersvc.predict(test_vectors_level_A)\n",
    "test_predictions_A_mnb = classifiermnb.predict(test_vectors_level_A)\n",
    "test_predictions_A_sgd = classifiersgd.predict(test_vectors_level_A)\n",
    "test_predictions_A_mlp = classifiermlp.predict(test_vectors_level_A)\n",
    "\n",
    "\n",
    "accuracy_svc = accuracy_score(test_labels_level_A, test_predictions_A_svc)\n",
    "accuracy_mnb = accuracy_score(test_labels_level_A, test_predictions_A_mnb)\n",
    "accuracy_sgd = accuracy_score(test_labels_level_A, test_predictions_A_sgd)\n",
    "accuracy_mlp = accuracy_score(test_labels_level_A, test_predictions_A_mlp)\n",
    "print(\"Level A: \\n\")\n",
    "print(\"Test Accuracy SVC:\", accuracy_svc)\n",
    "print(\"Test Accuracy MNB:\", accuracy_mnb)\n",
    "print(\"Test Accuracy SGD:\", accuracy_sgd)\n",
    "print(\"Test Accuracy MLP:\", accuracy_mlp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preds_A = pd.DataFrame(columns = ['SVC_A', 'MNB_A', 'SGD_A','MLP_A'])\n",
    "label_name = ['SVC_A', 'MNB_A','SGD_A','MLP_A']\n",
    "preds = [test_predictions_A_svc, test_predictions_A_mnb, test_predictions_A_sgd, test_predictions_A_mlp]\n",
    "\n",
    "i = 0\n",
    "for label in label_name:\n",
    "    preds_A[label] = preds[i]\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split into Train and Test vectors using the vectors of level A and Labels of level B with a training size of 0.75.\n",
    "train_vectors_level_B, test_vectors_level_B, train_labels_level_B, test_labels_level_B = train_test_split(vectors_level_B[:], labels_level_b[:], train_size=0.75)\n",
    "\n",
    "## Extracting names of labels and storing them in a variable\n",
    "classNames = np.unique(test_labels_level_B)\n",
    "print(\"Training begins on Level B classification...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "## Creating an object of SVC\n",
    "classifiersvc = SVC()\n",
    "classifiermnb = MultinomialNB()\n",
    "classifiersgd = SGDClassifier()\n",
    "classifiermlp = MLPClassifier()\n",
    "\n",
    "classifiersvc.fit(train_vectors_level_B, train_labels_level_B)\n",
    "classifiermnb.fit(train_vectors_level_B, train_labels_level_B)\n",
    "classifiersgd.fit(train_vectors_level_B, train_labels_level_B)\n",
    "classifiermlp.fit(train_vectors_level_B, train_labels_level_B)\n",
    "\n",
    "print(\"Training complete....\")\n",
    "\n",
    "\n",
    "print(\"calculating accuracy....\")\n",
    "## Training accuracy has been calculated\n",
    "accuracy = accuracy_score(train_labels_level_B, classifiersvc.predict(train_vectors_level_B))\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "\n",
    "## predictions are obtained on the testing data set\n",
    "test_predictions_B_svc = classifiersvc.predict(test_vectors_level_B)\n",
    "test_predictions_B_mnb = classifiermnb.predict(test_vectors_level_B)\n",
    "test_predictions_B_sgd = classifiersgd.predict(test_vectors_level_B)\n",
    "test_predictions_B_mlp = classifiermlp.predict(test_vectors_level_B)\n",
    "\n",
    "## Testing accuracy has been calculated\n",
    "accuracy_svc = accuracy_score(test_labels_level_B, test_predictions_B_svc)\n",
    "accuracy_mnb = accuracy_score(test_labels_level_B, test_predictions_B_mnb)\n",
    "accuracy_sgd = accuracy_score(test_labels_level_B, test_predictions_B_sgd)\n",
    "accuracy_mlp = accuracy_score(test_labels_level_B, test_predictions_B_mlp)\n",
    "print(\"Level B: \\n\")\n",
    "print(\"Test Accuracy SVC:\", accuracy_svc)\n",
    "print(\"Test Accuracy MNB:\", accuracy_mnb)\n",
    "print(\"Test Accuracy SGD:\", accuracy_sgd)\n",
    "print(\"Test Accuracy MLP:\", accuracy_mlp)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "## confusion matrix has been obtained for level A classification\n",
    "matrix_level_B = confusion_matrix(test_labels_level_B, test_predictionsB)\n",
    "print(matrix_level_B)\n",
    "## Obtaining classification report for the test data set\n",
    "print(classification_report(test_labels_level_B,test_predictionsB))\n",
    "\n",
    "# ## Plotting confusion matrix for better visualization\n",
    "# plottedCM_Level_B = plot_confusion_matrix(classifiersvc, test_vectors_level_B, test_labels_level_B, display_labels=classNames, cmap=plt.cm.Blues)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Split into Train and Test vectors using the vectors of level A and Labels of level B with a training size of 0.75.\n",
    "train_vectors_level_C, test_vectors_level_C, train_labels_level_C, test_labels_level_C = train_test_split(vectors_level_c[:], labels_level_c[:], train_size=0.75)\n",
    "\n",
    "## Extracting names of labels and storing them in a variable\n",
    "classNames = np.unique(test_labels_level_C)\n",
    "print(\"Training begins on Level C classification...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "## Creating an object of SVC\n",
    "classifiersvc = SVC()\n",
    "classifiermnb = MultinomialNB()\n",
    "classifiersgd = SGDClassifier()\n",
    "classifiermlp = MLPClassifier()\n",
    "\n",
    "classifiersvc.fit(train_vectors_level_C, train_labels_level_C)\n",
    "classifiermnb.fit(train_vectors_level_C, train_labels_level_C)\n",
    "classifiersgd.fit(train_vectors_level_C, train_labels_level_C)\n",
    "classifiermlp.fit(train_vectors_level_C, train_labels_level_C)\n",
    "\n",
    "\n",
    "\n",
    "## predictions are obtained on the testing data set\n",
    "test_predictions_C_svc = classifiersvc.predict(test_vectors_level_C)\n",
    "test_predictions_C_mnb = classifiermnb.predict(test_vectors_level_C)\n",
    "test_predictions_C_sgd = classifiersgd.predict(test_vectors_level_C)\n",
    "test_predictions_C_mlp = classifiermlp.predict(test_vectors_level_C)\n",
    "\n",
    "\n",
    "accuracy_svc = accuracy_score(test_labels_level_C, test_predictions_C_svc)\n",
    "accuracy_mnb = accuracy_score(test_labels_level_C, test_predictions_C_mnb)\n",
    "accuracy_sgd = accuracy_score(test_labels_level_C, test_predictions_C_sgd)\n",
    "accuracy_mlp = accuracy_score(test_labels_level_C, test_predictions_C_mlp)\n",
    "print(\"Level C: \\n\")\n",
    "print(\"Test Accuracy SVC:\", accuracy_svc)\n",
    "print(\"Test Accuracy MNB:\", accuracy_mnb)\n",
    "print(\"Test Accuracy SGD:\", accuracy_sgd)\n",
    "print(\"Test Accuracy MLP:\", accuracy_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_B = pd.DataFrame(columns = ['SVC_B', 'MNB_B', 'SGD_B','MLP_B'])\n",
    "label_name = ['SVC_B', 'MNB_B','SGD_B','MLP_B']\n",
    "preds = [test_predictions_B_svc, test_predictions_B_mnb, test_predictions_B_sgd, test_predictions_B_mlp]\n",
    "\n",
    "i = 0\n",
    "for label in label_name:\n",
    "    preds_B[label] = preds[i]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_C = pd.DataFrame(columns = ['SVC_C', 'MNB_C', 'SGD_C','MLP_C'])\n",
    "label_name = ['SVC_C', 'MNB_C','SGD_C','MLP_C']\n",
    "preds = [test_predictions_C_svc, test_predictions_C_mnb, test_predictions_C_sgd, test_predictions_C_mlp]\n",
    "\n",
    "i = 0\n",
    "for label in label_name:\n",
    "    preds_C[label] = preds[i]\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df = pd.DataFrame()\n",
    "final_df = pd.concat([preds_A, preds_B, preds_C], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"/Users/mayureshnene/Desktop/Mayuresh/Offense_Marathi/Experiments/Data_annotated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
