{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the local git repo for Indic NLP library\n",
    "INDIC_NLP_LIB_HOME=r\"/Users/mayureshnene/Desktop/indic_nlp_library\"\n",
    "\n",
    "# The path to the local git repo for Indic NLP Resources\n",
    "INDIC_NLP_RESOURCES=r\"/Users/mayureshnene/Desktop/indic_nlp_resources\"\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from indicnlp import common\n",
    "from indicnlp import loader\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from indicnlp.tokenize import indic_tokenize \n",
    "from indicnlp.morph import unsupervised_morph \n",
    "from indicnlp import common\n",
    "\n",
    "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
    "\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
    "\n",
    "loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        NULL\n",
       "1        NULL\n",
       "2        NULL\n",
       "3        NULL\n",
       "4        NULL\n",
       "         ... \n",
       "12990    NULL\n",
       "12991    NULL\n",
       "12992    NULL\n",
       "12993    NULL\n",
       "12994    NULL\n",
       "Name: subtask_c, Length: 12995, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = pd.read_csv(\"/Users/mayureshnene/Desktop/Mayuresh/Offense_Marathi/Data/Data.csv\")\n",
    "training_dataset.dropna()\n",
    "training_dataset['subtask_c'].fillna(\"NULL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        आजच्या जनता दरबारात जळगाव जिल्ह्यातील चाळीसगाव...\n",
       "1        कुणी कविता करत असतं तर कुणी कविता जगत असतं कुण...\n",
       "2        आम्हाला इतिहासातील औरंगजेबशी काही घेणे नाही आम...\n",
       "3        गँभीर प्रकरण महाराष्ट्राची अवस्था बिकट आहे भाष...\n",
       "4        कब्झा हा कन्नड चित्रपट लवकरच मराठी मध्ये डब्ब ...\n",
       "                               ...                        \n",
       "12990    पहिल्या लाटे नंतर सर्वात कमी रुग्ण\\n#कोरोना #र...\n",
       "12991    RT @DeshRat30997789: @RavindraAmbekar जगातले स...\n",
       "12992    Tata Tigor EV | देशातील सर्वात स्वस्त इलेक्ट्र...\n",
       "12993    RT @PIBMumbai: पंतप्रधान आयुष्मान भारत आरोग्यव...\n",
       "12994    @Hopeful_Addict काटकसर म्हणजे गोधड्या शिवणे.\\n...\n",
       "Name: tweet, Length: 12995, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = training_dataset[\"tweet\"]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_A = training_dataset[[\"subtask_a\"]]\n",
    "# level_B = training_dataset.query(\"subtask_a == 'Offensive'\")[[\"subtask_b\"]]\n",
    "# level_C = training_dataset.query(\"subtask_b == 'TIN'\")[[\"subtask_c\"]]\n",
    "level_B_indices = training_dataset.index[training_dataset['subtask_a'] == \"Offensive\"].tolist()\n",
    "level_C_indices = training_dataset.index[training_dataset['subtask_b'] == \"TIN\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_B = training_dataset[\"subtask_b\"][level_B_indices]\n",
    "level_C = training_dataset[\"subtask_c\"][level_C_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1451    GRP\n",
       "1452    GRP\n",
       "1453    GRP\n",
       "1454    GRP\n",
       "1455    GRP\n",
       "       ... \n",
       "3057    OTH\n",
       "3092    OTH\n",
       "3093    OTH\n",
       "3094    OTH\n",
       "3095    OTH\n",
       "Name: subtask_c, Length: 739, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['अधिक ', 'अनेक ', 'अशी ', 'असलयाचे', 'असलेल्या', 'असा', 'असून', 'असे', 'आज', 'आणि', 'आता', 'आपल्या', 'आला', 'आली', 'आले', 'आहे', 'आहेत', 'एक', 'एका', 'कमी', 'करणयात', 'करून', 'का', 'काम', 'काय', 'काही', 'किवा', 'की', 'केला', 'केली', 'केले', 'कोटी', 'गेल्या', 'घेऊन', 'जात', 'झाला', 'झाली', 'झाले', 'झालेल्या', 'टा', 'डॉ', 'तर', 'तरी', 'तसेच', 'ता', 'ती', 'तीन', 'ते', 'तो', 'त्या', 'त्याचा', 'त्याची', 'त्याच्या', 'त्याना', 'त्यानी', 'त्यामुळे', 'त्री', 'दिली', 'दोन', 'न', 'नाही', 'निर्ण्य', 'पण', 'पम', 'परयतन', 'पाटील', 'म', 'मात्र', 'माहिती', 'मी', 'मुबी', 'म्हणजे', 'म्हणाले', 'म्हणून', 'या', 'याचा', 'याची', 'याच्या', 'याना', 'यानी', 'येणार', 'येत', 'येथील', 'येथे', 'लाख', 'व', 'व्यकत', 'सर्व', 'सागित्ले', 'सुरू', 'हजार', 'हा', 'ही', 'हे', 'होणार', 'होत', 'होता', 'होती', 'होते']\n"
     ]
    }
   ],
   "source": [
    "stopwords_file = open(\"/Users/mayureshnene/Desktop/MOLD/Mold/stopwords.txt\")\n",
    "stopwords = stopwords_file.read().splitlines()\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(row):\n",
    "    row = str(row)\n",
    "#     removal_list = ['URL','\\'ve','n\\'t','\\'s','\\'m','!']\n",
    "#     for element in removal_list:\n",
    "#         row = row.replace(element,'')\n",
    "    \n",
    "    row = row.replace('http\\S+|www.\\S+', '')\n",
    "    row = re.sub(\"@[A-Za-z0-9]+\",\"@USER\",row)\n",
    "    row = re.sub(\"[A-Za-z0-9]+\",\"\",row)\n",
    "    row = re.sub(\"@\",\"@USER\",row)\n",
    "    row = re.sub('[+,-,_,=,/,<,>,!,#,$,%,^,&,*,\\\",:,;,.,' ',\\t,\\r,\\n,\\',|]','',row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tweet in tweets:\n",
    "#     tweet = remove_noise(str(tweet))\n",
    "iterator_map = map(clean,tweets)\n",
    "tweets = list(iterator_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "collective_tweets = copy.deepcopy(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "analyzer=unsupervised_morph.UnsupervisedMorphAnalyzer('mr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    return indic_tokenize.trivial_tokenize(tweet)\n",
    "\n",
    "def morph(tweet):\n",
    "    analyzed_tokens=analyzer.morph_analyze_document(str(tweet).split(' '))\n",
    "    return analyzed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenize..: 100%|██████████| 12995/12995 [00:03<00:00, 4323.81it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"Tokenize..\")\n",
    "#all_tweets[\"tokens\"] = all_tweets['tweet'].progress_apply(tokenize)\n",
    "collective_tweets[\"tokens\"] = collective_tweets['tweet'].progress_apply(morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = collective_tweets[\"tokens\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfid_vectorizer(vector):\n",
    "\t## Creates and stores an instance of the TfidfVectorizer class. This will be used further to extract our data as tf-idf features.\n",
    "\tvectorizer = TfidfVectorizer()\n",
    "\tuntokenized_data =[' '.join(tweet) for tweet in tqdm(vector, \"Vectorizing...\")]\n",
    "\tvectorizer = vectorizer.fit(untokenized_data)\n",
    "\tvectors = vectorizer.transform(untokenized_data).toarray()\n",
    "\treturn vectors\n",
    "\n",
    "def get_vectors(vectors, labels, keyword):\n",
    "\t'''\n",
    "\tReturns a matrix for vectors. Zips vectors and labels IF and only if length of vector list is the same as length of the labels list. \n",
    "\tElse, the function gets terminated.\n",
    "\t@param vectors These are the vectors for a given label.\n",
    "\t@param labels These are the label values for the given label.\n",
    "\t@param keyword which is the label to annotate for.\n",
    "\t'''\n",
    "\tif len(vectors) != len(labels):\n",
    "\t\tprint(\"Unmatching sizes!\")\n",
    "\t\treturn\n",
    "\t\n",
    "\t## Stores a new list to append the zipped vectors and labels into.\n",
    "\tresult = list()\n",
    "\tfor vector, label in zip(vectors, labels):\n",
    "\t\tif label == keyword:\n",
    "\t\t\tresult.append(vector)\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing...: 100%|██████████| 12995/12995 [00:00<00:00, 673949.36it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors_level_A = tfid_vectorizer(vector)\n",
    "labels_level_a = level_A['subtask_a'].values.tolist()\n",
    "vectors_level_B = get_vectors(vectors_level_A, labels_level_a, \"Offensive\") \n",
    "\n",
    "labels_level_b = level_B['subtask_b'].values.tolist() \n",
    "\n",
    "## Numerical Vectors C\n",
    "vectors_level_c = get_vectors(vectors_level_B, labels_level_b, \"TIN\") \n",
    "\n",
    "##Subtask C Labels\n",
    "labels_level_c = level_C['subtask_c'].values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begins on Level A classification...\n"
     ]
    }
   ],
   "source": [
    "train_vectors_level_A, train_labels_level_A,= vectors_level_A[1:3097], labels_level_a[1:3097]\n",
    "test_vectors_level_A, test_labels_level_A = vectors_level_A[3097:], labels_level_a[3097:]\n",
    "## Extracting names of labels and storing them in a variable\n",
    "classNames = np.unique(test_labels_level_A)\n",
    "\n",
    "print(\"Training begins on Level A classification...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "## Creating an object of SVC\n",
    "classifiersvc = SVC()\n",
    "classifiermnb = MultinomialNB()\n",
    "classifiersgd = SGDClassifier()\n",
    "classifiermlp = MLPClassifier()\n",
    "\n",
    "classifiersvc.fit(train_vectors_level_A, train_labels_level_A)\n",
    "classifiermnb.fit(train_vectors_level_A, train_labels_level_A)\n",
    "classifiersgd.fit(train_vectors_level_A, train_labels_level_A)\n",
    "classifiermlp.fit(train_vectors_level_A, train_labels_level_A)\n",
    "\n",
    "# accuracy = accuracy_score(train_labels_level_A, classifiersvc.predict(train_vectors_level_A))\n",
    "# print(\"Training Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "test_predictions_A_svc = classifiersvc.predict(test_vectors_level_A)\n",
    "test_predictions_A_mnb = classifiermnb.predict(test_vectors_level_A)\n",
    "test_predictions_A_sgd = classifiersgd.predict(test_vectors_level_A)\n",
    "test_predictions_A_mlp = classifiermlp.predict(test_vectors_level_A)\n",
    "\n",
    "\n",
    "# accuracy_svc = accuracy_score(test_labels_level_A, test_predictions_A_svc)\n",
    "# accuracy_mnb = accuracy_score(test_labels_level_A, test_predictions_A_mnb)\n",
    "# accuracy_sgd = accuracy_score(test_labels_level_A, test_predictions_A_sgd)\n",
    "# accuracy_mlp = accuracy_score(test_labels_level_A, test_predictions_A_mlp)\n",
    "# print(\"Level A: \\n\")\n",
    "# print(\"Test Accuracy SVC:\", accuracy_svc)\n",
    "# print(\"Test Accuracy MNB:\", accuracy_mnb)\n",
    "# print(\"Test Accuracy SGD:\", accuracy_sgd)\n",
    "# print(\"Test Accuracy MLP:\", accuracy_mlp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preds_A = pd.DataFrame(columns = ['SVC_A', 'MNB_A', 'SGD_A','MLP_A'])\n",
    "label_name = ['SVC_A', 'MNB_A','SGD_A','MLP_A']\n",
    "preds = [test_predictions_A_svc, test_predictions_A_mnb, test_predictions_A_sgd, test_predictions_A_mlp]\n",
    "\n",
    "i = 0\n",
    "for label in label_name:\n",
    "    preds_A[label] = preds[i]\n",
    "    i += 1\n",
    "\n",
    "\n",
    "preds_A.to_csv(\"/Users/mayureshnene/Desktop/Mayuresh/Offense_Marathi/Experiments/Data_annotated.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begins on Level B classification...\n",
      "Training complete....\n",
      "calculating accuracy....\n",
      "Training Accuracy: 0.8845671267252195\n",
      "Training begins on Level C classification...\n"
     ]
    }
   ],
   "source": [
    "# ## Split into Train and Test vectors using the vectors of level A and Labels of level B with a training size of 0.75.\n",
    "# train_vectors_level_B, test_vectors_level_B, train_labels_level_B, test_labels_level_B = train_test_split(vectors_level_B[:], labels_level_b[:], train_size=0.75)\n",
    "\n",
    "# ## Extracting names of labels and storing them in a variable\n",
    "# classNames = np.unique(test_labels_level_B)\n",
    "# print(\"Training begins on Level B classification...\")\n",
    "# warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# ## Creating an object of SVC\n",
    "# classifiersvc = SVC()\n",
    "# classifiermnb = MultinomialNB()\n",
    "# classifiersgd = SGDClassifier()\n",
    "# classifiermlp = MLPClassifier()\n",
    "\n",
    "# classifiersvc.fit(train_vectors_level_B, train_labels_level_B)\n",
    "# classifiermnb.fit(train_vectors_level_B, train_labels_level_B)\n",
    "# classifiersgd.fit(train_vectors_level_B, train_labels_level_B)\n",
    "# classifiermlp.fit(train_vectors_level_B, train_labels_level_B)\n",
    "\n",
    "# print(\"Training complete....\")\n",
    "\n",
    "\n",
    "# print(\"calculating accuracy....\")\n",
    "# ## Training accuracy has been calculated\n",
    "# accuracy = accuracy_score(train_labels_level_B, classifiersvc.predict(train_vectors_level_B))\n",
    "# print(\"Training Accuracy:\", accuracy)\n",
    "\n",
    "# ## predictions are obtained on the testing data set\n",
    "# test_predictions_B_svc = classifiersvc.predict(test_vectors_level_B)\n",
    "# test_predictions_B_mnb = classifiermnb.predict(test_vectors_level_B)\n",
    "# test_predictions_B_sgd = classifiersgd.predict(test_vectors_level_B)\n",
    "# test_predictions_B_mlp = classifiermlp.predict(test_vectors_level_B)\n",
    "\n",
    "# ## Testing accuracy has been calculated\n",
    "# # accuracy_svc = accuracy_score(test_labels_level_B, test_predictions_B_svc)\n",
    "# # accuracy_mnb = accuracy_score(test_labels_level_B, test_predictions_B_mnb)\n",
    "# # accuracy_sgd = accuracy_score(test_labels_level_B, test_predictions_B_sgd)\n",
    "# # accuracy_mlp = accuracy_score(test_labels_level_B, test_predictions_B_mlp)\n",
    "# # print(\"Level B: \\n\")\n",
    "# # print(\"Test Accuracy SVC:\", accuracy_svc)\n",
    "# # print(\"Test Accuracy MNB:\", accuracy_mnb)\n",
    "# # print(\"Test Accuracy SGD:\", accuracy_sgd)\n",
    "# # print(\"Test Accuracy MLP:\", accuracy_mlp)\n",
    "\n",
    "\n",
    "# ## Split into Train and Test vectors using the vectors of level A and Labels of level B with a training size of 0.75.\n",
    "# train_vectors_level_C, test_vectors_level_C, train_labels_level_C, test_labels_level_C = train_test_split(vectors_level_c[:], labels_level_c[:], train_size=0.75)\n",
    "\n",
    "# ## Extracting names of labels and storing them in a variable\n",
    "# classNames = np.unique(test_labels_level_C)\n",
    "# print(\"Training begins on Level C classification...\")\n",
    "# warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# ## Creating an object of SVC\n",
    "# classifiersvc = SVC()\n",
    "# classifiermnb = MultinomialNB()\n",
    "# classifiersgd = SGDClassifier()\n",
    "# classifiermlp = MLPClassifier()\n",
    "\n",
    "# classifiersvc.fit(train_vectors_level_C, train_labels_level_C)\n",
    "# classifiermnb.fit(train_vectors_level_C, train_labels_level_C)\n",
    "# classifiersgd.fit(train_vectors_level_C, train_labels_level_C)\n",
    "# classifiermlp.fit(train_vectors_level_C, train_labels_level_C)\n",
    "\n",
    "\n",
    "\n",
    "# ## predictions are obtained on the testing data set\n",
    "# test_predictions_C_svc = classifiersvc.predict(test_vectors_level_C)\n",
    "# test_predictions_C_mnb = classifiermnb.predict(test_vectors_level_C)\n",
    "# test_predictions_C_sgd = classifiersgd.predict(test_vectors_level_C)\n",
    "# test_predictions_C_mlp = classifiermlp.predict(test_vectors_level_C)\n",
    "\n",
    "\n",
    "# # accuracy_svc = accuracy_score(test_labels_level_C, test_predictions_C_svc)\n",
    "# # accuracy_mnb = accuracy_score(test_labels_level_C, test_predictions_C_mnb)\n",
    "# # accuracy_sgd = accuracy_score(test_labels_level_C, test_predictions_C_sgd)\n",
    "# # accuracy_mlp = accuracy_score(test_labels_level_C, test_predictions_C_mlp)\n",
    "# # print(\"Level C: \\n\")\n",
    "# # print(\"Test Accuracy SVC:\", accuracy_svc)\n",
    "# # print(\"Test Accuracy MNB:\", accuracy_mnb)\n",
    "# # print(\"Test Accuracy SGD:\", accuracy_sgd)\n",
    "# # print(\"Test Accuracy MLP:\", accuracy_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_B = pd.DataFrame(columns = ['SVC_B', 'MNB_B', 'SGD_B','MLP_B'])\n",
    "# label_name = ['SVC_B', 'MNB_B','SGD_B','MLP_B']\n",
    "# preds = [test_predictions_B_svc, test_predictions_B_mnb, test_predictions_B_sgd, test_predictions_B_mlp]\n",
    "\n",
    "# i = 0\n",
    "# for label in label_name:\n",
    "#     preds_B[label] = preds[i]\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_C = pd.DataFrame(columns = ['SVC_C', 'MNB_C', 'SGD_C','MLP_C'])\n",
    "# label_name = ['SVC_C', 'MNB_C','SGD_C','MLP_C']\n",
    "# preds = [test_predictions_C_svc, test_predictions_C_mnb, test_predictions_C_sgd, test_predictions_C_mlp]\n",
    "\n",
    "# i = 0\n",
    "# for label in label_name:\n",
    "#     preds_C[label] = preds[i]\n",
    "#     i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #final_df = pd.DataFrame()\n",
    "# final_df = pd.concat([preds_A, preds_B, preds_C], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVC_A</th>\n",
       "      <th>MNB_A</th>\n",
       "      <th>SGD_A</th>\n",
       "      <th>MLP_A</th>\n",
       "      <th>SVC_B</th>\n",
       "      <th>MNB_B</th>\n",
       "      <th>SGD_B</th>\n",
       "      <th>MLP_B</th>\n",
       "      <th>SVC_C</th>\n",
       "      <th>MNB_C</th>\n",
       "      <th>SGD_C</th>\n",
       "      <th>MLP_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10344</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IND</td>\n",
       "      <td>IND</td>\n",
       "      <td>IND</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IND</td>\n",
       "      <td>IND</td>\n",
       "      <td>IND</td>\n",
       "      <td>OTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10346</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IND</td>\n",
       "      <td>IND</td>\n",
       "      <td>IND</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10347</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IND</td>\n",
       "      <td>IND</td>\n",
       "      <td>IND</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10348</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GRP</td>\n",
       "      <td>IND</td>\n",
       "      <td>GRP</td>\n",
       "      <td>OTH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10349 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SVC_A MNB_A SGD_A MLP_A SVC_B MNB_B SGD_B MLP_B SVC_C MNB_C SGD_C MLP_C\n",
       "0       nan   nan   nan   nan   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN\n",
       "1       nan   nan   nan   nan   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN\n",
       "2       nan   nan   nan   nan   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN\n",
       "3       nan   nan   nan   nan   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN\n",
       "4       nan   nan   nan   nan   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN\n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...\n",
       "10344   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   IND   IND   IND   IND\n",
       "10345   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   IND   IND   IND   OTH\n",
       "10346   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   IND   IND   IND   IND\n",
       "10347   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   IND   IND   IND   IND\n",
       "10348   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   GRP   IND   GRP   OTH\n",
       "\n",
       "[10349 rows x 12 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.to_csv(\"/Users/mayureshnene/Desktop/Mayuresh/Offense_Marathi/Experiments/Data_annotated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
