{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the local git repo for Indic NLP library\n",
    "INDIC_NLP_LIB_HOME=r\"/Users/mayureshnene/Desktop/indic_nlp_library\"\n",
    "\n",
    "# The path to the local git repo for Indic NLP Resources\n",
    "INDIC_NLP_RESOURCES=r\"/Users/mayureshnene/Desktop/indic_nlp_resources\"\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from indicnlp import common\n",
    "from indicnlp import loader\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from indicnlp.tokenize import indic_tokenize \n",
    "from indicnlp.morph import unsupervised_morph \n",
    "from indicnlp import common\n",
    "\n",
    "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
    "\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
    "\n",
    "loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12995, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = pd.read_csv(\"/Users/mayureshnene/Desktop/Mayuresh/Offense_Marathi/Data/Data.csv\")\n",
    "training_dataset.head()\n",
    "training_dataset.shape\n",
    "#training_dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        पूर्व लडाखमधील मुखपरी येथे प्रत्यक्ष ताबारेषे...\n",
       "1        कोणत्याही succesful रिलेशनशिप मध्ये सुंदर दिस...\n",
       "2       भारत 15 ऑगस्ट 1947 ला स्वतंत्र झाला आणि त्यानं...\n",
       "3       स्वत ला हवा तसा बाइट किंवा प्रतिक्रिया घेण्यास...\n",
       "4       5 व्या नंबरची अर्थव्यवस्था आहे भारताची जगात 20...\n",
       "                              ...                        \n",
       "2692    @USER अहो त्याचा रोजगार गेला ना तो महिन्याला ४...\n",
       "2693    @USER चंपा जी ब्लॉक करुन जाणारे भुरटे धड निवडण...\n",
       "2694    चंपा जी ब्लॉक करुन जाणारे भुरटे धड निवडणून येत...\n",
       "2695                                            @USER मंद\n",
       "2696    केली का काय होते ते सांगा या झमन्याला आणि चेंब...\n",
       "Name: tweet, Length: 2697, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = training_dataset[\"tweet\"]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_A = training_dataset[[\"subtask_a\"]]\n",
    "level_B = training_dataset.query(\"subtask_a == 'Offensive'\")[[\"subtask_b\"]]\n",
    "level_C = training_dataset.query(\"subtask_b == 'TIN'\")[[\"subtask_c\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['अधिक ', 'अनेक ', 'अशी ', 'असलयाचे', 'असलेल्या', 'असा', 'असून', 'असे', 'आज', 'आणि', 'आता', 'आपल्या', 'आला', 'आली', 'आले', 'आहे', 'आहेत', 'एक', 'एका', 'कमी', 'करणयात', 'करून', 'का', 'काम', 'काय', 'काही', 'किवा', 'की', 'केला', 'केली', 'केले', 'कोटी', 'गेल्या', 'घेऊन', 'जात', 'झाला', 'झाली', 'झाले', 'झालेल्या', 'टा', 'डॉ', 'तर', 'तरी', 'तसेच', 'ता', 'ती', 'तीन', 'ते', 'तो', 'त्या', 'त्याचा', 'त्याची', 'त्याच्या', 'त्याना', 'त्यानी', 'त्यामुळे', 'त्री', 'दिली', 'दोन', 'न', 'नाही', 'निर्ण्य', 'पण', 'पम', 'परयतन', 'पाटील', 'म', 'मात्र', 'माहिती', 'मी', 'मुबी', 'म्हणजे', 'म्हणाले', 'म्हणून', 'या', 'याचा', 'याची', 'याच्या', 'याना', 'यानी', 'येणार', 'येत', 'येथील', 'येथे', 'लाख', 'व', 'व्यकत', 'सर्व', 'सागित्ले', 'सुरू', 'हजार', 'हा', 'ही', 'हे', 'होणार', 'होत', 'होता', 'होती', 'होते']\n"
     ]
    }
   ],
   "source": [
    "stopwords_file = open(\"/Users/mayureshnene/Desktop/MOLD/Mold/stopwords.txt\")\n",
    "stopwords = stopwords_file.read().splitlines()\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(row):\n",
    "    row = str(row)\n",
    "#     removal_list = ['URL','\\'ve','n\\'t','\\'s','\\'m','!']\n",
    "#     for element in removal_list:\n",
    "#         row = row.replace(element,'')\n",
    "    \n",
    "    row = row.replace('http\\S+|www.\\S+', '')\n",
    "    row = re.sub(\"@[A-Za-z0-9]+\",\"@USER\",row)\n",
    "    row = re.sub(\"[A-Za-z0-9]+\",\"\",row)\n",
    "    row = re.sub(\"@\",\"@USER\",row)\n",
    "    row = re.sub('[+,-,_,=,/,<,>,!,#,$,%,^,&,*,\\\",:,;,.,' ',\\t,\\r,\\n,\\',|]','',row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tweet in tweets:\n",
    "#     tweet = remove_noise(str(tweet))\n",
    "iterator_map = map(clean,tweets)\n",
    "tweets = list(iterator_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "collective_tweets = copy.deepcopy(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "analyzer=unsupervised_morph.UnsupervisedMorphAnalyzer('mr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    return indic_tokenize.trivial_tokenize(tweet)\n",
    "\n",
    "def morph(tweet):\n",
    "    analyzed_tokens=analyzer.morph_analyze_document(str(tweet).split(' '))\n",
    "    return analyzed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenize..: 100%|██████████| 2697/2697 [00:00<00:00, 3331.34it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"Tokenize..\")\n",
    "#all_tweets[\"tokens\"] = all_tweets['tweet'].progress_apply(tokenize)\n",
    "collective_tweets[\"tokens\"] = collective_tweets['tweet'].progress_apply(morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = collective_tweets[\"tokens\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfid_vectorizer(vector):\n",
    "\t## Creates and stores an instance of the TfidfVectorizer class. This will be used further to extract our data as tf-idf features.\n",
    "\tvectorizer = TfidfVectorizer()\n",
    "\tuntokenized_data =[' '.join(tweet) for tweet in tqdm(vector, \"Vectorizing...\")]\n",
    "\tvectorizer = vectorizer.fit(untokenized_data)\n",
    "\tvectors = vectorizer.transform(untokenized_data).toarray()\n",
    "\treturn vectors\n",
    "\n",
    "def get_vectors(vectors, labels, keyword):\n",
    "\t'''\n",
    "\tReturns a matrix for vectors. Zips vectors and labels IF and only if length of vector list is the same as length of the labels list. \n",
    "\tElse, the function gets terminated.\n",
    "\t@param vectors These are the vectors for a given label.\n",
    "\t@param labels These are the label values for the given label.\n",
    "\t@param keyword which is the label to annotate for.\n",
    "\t'''\n",
    "\tif len(vectors) != len(labels):\n",
    "\t\tprint(\"Unmatching sizes!\")\n",
    "\t\treturn\n",
    "\t\n",
    "\t## Stores a new list to append the zipped vectors and labels into.\n",
    "\tresult = list()\n",
    "\tfor vector, label in zip(vectors, labels):\n",
    "\t\tif label == keyword:\n",
    "\t\t\tresult.append(vector)\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing...: 100%|██████████| 2697/2697 [00:00<00:00, 511001.40it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors_level_A = tfid_vectorizer(vector)\n",
    "labels_level_a = level_A['subtask_a'].values.tolist()\n",
    "vectors_level_B = get_vectors(vectors_level_A, labels_level_a, \"Offensive\") \n",
    "\n",
    "labels_level_b = level_B['subtask_b'].values.tolist() \n",
    "\n",
    "## Numerical Vectors C\n",
    "vectors_level_c = get_vectors(vectors_level_B, labels_level_b, \"TIN\") \n",
    "\n",
    "##Subtask C Labels\n",
    "labels_level_c = level_C['subtask_c'].values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begins on Level A classification...\n",
      "Training Accuracy: 0.9350890207715133\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1724)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e077c9d0460e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtest_predictions_A_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifiersvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vectors_level_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mtest_predictions_A_mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifiermnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vectors_level_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtest_predictions_A_sgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifiersgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vectors_level_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \"\"\"\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             X = check_array(X, accept_sparse='csr', dtype=np.float64,\n\u001b[0m\u001b[1;32m    475\u001b[0m                             order=\"C\", accept_large_sparse=False)\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n\u001b[0m\u001b[1;32m    670\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1724)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "train_vectors_level_A, train_labels_level_A,= vectors_level_A[1:], labels_level_a[1:]\n",
    "test_vectors_level_A, test_labels_level_A = vectors_level_A[3097:], labels_level_a[3097:]\n",
    "## Extracting names of labels and storing them in a variable\n",
    "classNames = np.unique(test_labels_level_A)\n",
    "\n",
    "print(\"Training begins on Level A classification...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "## Creating an object of SVC\n",
    "classifiersvc = SVC()\n",
    "classifiermnb = MultinomialNB()\n",
    "classifiersgd = SGDClassifier()\n",
    "classifiermlp = MLPClassifier()\n",
    "\n",
    "classifiersvc.fit(train_vectors_level_A, train_labels_level_A)\n",
    "classifiermnb.fit(train_vectors_level_A, train_labels_level_A)\n",
    "classifiersgd.fit(train_vectors_level_A, train_labels_level_A)\n",
    "classifiermlp.fit(train_vectors_level_A, train_labels_level_A)\n",
    "\n",
    "# accuracy = accuracy_score(train_labels_level_A, classifiersvc.predict(train_vectors_level_A))\n",
    "# print(\"Training Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "test_predictions_A_svc = classifiersvc.predict(test_vectors_level_A)\n",
    "test_predictions_A_mnb = classifiermnb.predict(test_vectors_level_A)\n",
    "test_predictions_A_sgd = classifiersgd.predict(test_vectors_level_A)\n",
    "test_predictions_A_mlp = classifiermlp.predict(test_vectors_level_A)\n",
    "\n",
    "\n",
    "accuracy_svc = accuracy_score(test_labels_level_B, test_predictions_B_svc)\n",
    "accuracy_mnb = accuracy_score(test_labels_level_B, test_predictions_B_mnb)\n",
    "accuracy_sgd = accuracy_score(test_labels_level_B, test_predictions_B_sgd)\n",
    "accuracy_mlp = accuracy_score(test_labels_level_B, test_predictions_B_mlp)\n",
    "print(\"Level B: \\n\")\n",
    "print(\"Test Accuracy SVC:\", accuracy_svc)\n",
    "print(\"Test Accuracy MNB:\", accuracy_mnb)\n",
    "print(\"Test Accuracy SGD:\", accuracy_sgd)\n",
    "print(\"Test Accuracy MLP:\", accuracy_mlp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preds_A = pd.DataFrame(columns = ['SVC_A', 'MNB_A', 'SGD_A','MLP_A'])\n",
    "label_name = ['SVC_A', 'MNB_A','SGD_A','MLP_A']\n",
    "preds = [test_predictions_A_svc, test_predictions_A_mnb, test_predictions_A_sgd, test_predictions_A_mlp]\n",
    "\n",
    "i = 0\n",
    "for label in label_name:\n",
    "    preds_A[label] = preds[i]\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begins on Level B classification...\n",
      "Training complete....\n",
      "calculating accuracy....\n",
      "Training Accuracy: 0.8835616438356164\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_predictionsB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f3514c05b6dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Confusion Matrix:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m## confusion matrix has been obtained for level A classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mmatrix_level_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels_level_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predictionsB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_level_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m## Obtaining classification report for the test data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_predictionsB' is not defined"
     ]
    }
   ],
   "source": [
    "## Split into Train and Test vectors using the vectors of level A and Labels of level B with a training size of 0.75.\n",
    "train_vectors_level_B, test_vectors_level_B, train_labels_level_B, test_labels_level_B = train_test_split(vectors_level_B[:], labels_level_b[:], train_size=0.75)\n",
    "\n",
    "## Extracting names of labels and storing them in a variable\n",
    "classNames = np.unique(test_labels_level_B)\n",
    "print(\"Training begins on Level B classification...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "## Creating an object of SVC\n",
    "classifiersvc = SVC()\n",
    "classifiermnb = MultinomialNB()\n",
    "classifiersgd = SGDClassifier()\n",
    "classifiermlp = MLPClassifier()\n",
    "\n",
    "classifiersvc.fit(train_vectors_level_B, train_labels_level_B)\n",
    "classifiermnb.fit(train_vectors_level_B, train_labels_level_B)\n",
    "classifiersgd.fit(train_vectors_level_B, train_labels_level_B)\n",
    "classifiermlp.fit(train_vectors_level_B, train_labels_level_B)\n",
    "\n",
    "print(\"Training complete....\")\n",
    "\n",
    "\n",
    "print(\"calculating accuracy....\")\n",
    "## Training accuracy has been calculated\n",
    "accuracy = accuracy_score(train_labels_level_B, classifiersvc.predict(train_vectors_level_B))\n",
    "print(\"Training Accuracy:\", accuracy)\n",
    "\n",
    "## predictions are obtained on the testing data set\n",
    "test_predictions_B_svc = classifiersvc.predict(test_vectors_level_B)\n",
    "test_predictions_B_mnb = classifiermnb.predict(test_vectors_level_B)\n",
    "test_predictions_B_sgd = classifiersgd.predict(test_vectors_level_B)\n",
    "test_predictions_B_mlp = classifiermlp.predict(test_vectors_level_B)\n",
    "\n",
    "## Testing accuracy has been calculated\n",
    "accuracy_svc = accuracy_score(test_labels_level_B, test_predictions_B_svc)\n",
    "accuracy_mnb = accuracy_score(test_labels_level_B, test_predictions_B_mnb)\n",
    "accuracy_sgd = accuracy_score(test_labels_level_B, test_predictions_B_sgd)\n",
    "accuracy_mlp = accuracy_score(test_labels_level_B, test_predictions_B_mlp)\n",
    "print(\"Level B: \\n\")\n",
    "print(\"Test Accuracy SVC:\", accuracy_svc)\n",
    "print(\"Test Accuracy MNB:\", accuracy_mnb)\n",
    "print(\"Test Accuracy SGD:\", accuracy_sgd)\n",
    "print(\"Test Accuracy MLP:\", accuracy_mlp)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "## confusion matrix has been obtained for level A classification\n",
    "matrix_level_B = confusion_matrix(test_labels_level_B, test_predictionsB)\n",
    "print(matrix_level_B)\n",
    "## Obtaining classification report for the test data set\n",
    "print(classification_report(test_labels_level_B,test_predictionsB))\n",
    "\n",
    "# ## Plotting confusion matrix for better visualization\n",
    "# plottedCM_Level_B = plot_confusion_matrix(classifiersvc, test_vectors_level_B, test_labels_level_B, display_labels=classNames, cmap=plt.cm.Blues)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Split into Train and Test vectors using the vectors of level A and Labels of level B with a training size of 0.75.\n",
    "train_vectors_level_C, test_vectors_level_C, train_labels_level_C, test_labels_level_C = train_test_split(vectors_level_c[:], labels_level_c[:], train_size=0.75)\n",
    "\n",
    "## Extracting names of labels and storing them in a variable\n",
    "classNames = np.unique(test_labels_level_C)\n",
    "print(\"Training begins on Level C classification...\")\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "## Creating an object of SVC\n",
    "classifiersvc = SVC()\n",
    "classifiermnb = MultinomialNB()\n",
    "classifiersgd = SGDClassifier()\n",
    "classifiermlp = MLPClassifier()\n",
    "\n",
    "classifiersvc.fit(train_vectors_level_C, train_labels_level_C)\n",
    "classifiermnb.fit(train_vectors_level_C, train_labels_level_C)\n",
    "classifiersgd.fit(train_vectors_level_C, train_labels_level_C)\n",
    "classifiermlp.fit(train_vectors_level_C, train_labels_level_C)\n",
    "\n",
    "\n",
    "\n",
    "## predictions are obtained on the testing data set\n",
    "test_predictions_C_svc = classifiersvc.predict(test_vectors_level_C)\n",
    "test_predictions_C_mnb = classifiermnb.predict(test_vectors_level_C)\n",
    "test_predictions_C_sgd = classifiersgd.predict(test_vectors_level_C)\n",
    "test_predictions_C_mlp = classifiermlp.predict(test_vectors_level_C)\n",
    "\n",
    "\n",
    "accuracy_svc = accuracy_score(test_labels_level_C, test_predictions_C_svc)\n",
    "accuracy_mnb = accuracy_score(test_labels_level_C, test_predictions_C_mnb)\n",
    "accuracy_sgd = accuracy_score(test_labels_level_C, test_predictions_C_sgd)\n",
    "accuracy_mlp = accuracy_score(test_labels_level_C, test_predictions_C_mlp)\n",
    "print(\"Level C: \\n\")\n",
    "print(\"Test Accuracy SVC:\", accuracy_svc)\n",
    "print(\"Test Accuracy MNB:\", accuracy_mnb)\n",
    "print(\"Test Accuracy SGD:\", accuracy_sgd)\n",
    "print(\"Test Accuracy MLP:\", accuracy_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_B = pd.DataFrame(columns = ['SVC_B', 'MNB_B', 'SGD_B','MLP_B'])\n",
    "label_name = ['SVC_B', 'MNB_B','SGD_B','MLP_B']\n",
    "preds = [test_predictions_B_svc, test_predictions_B_mnb, test_predictions_B_sgd, test_predictions_B_mlp]\n",
    "\n",
    "i = 0\n",
    "for label in label_name:\n",
    "    preds_B[label] = preds[i]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_C = pd.DataFrame(columns = ['SVC_C', 'MNB_C', 'SGD_C','MLP_C'])\n",
    "label_name = ['SVC_C', 'MNB_C','SGD_C','MLP_C']\n",
    "preds = [test_predictions_C_svc, test_predictions_C_mnb, test_predictions_C_sgd, test_predictions_C_mlp]\n",
    "\n",
    "i = 0\n",
    "for label in label_name:\n",
    "    preds_C[label] = preds[i]\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df = pd.DataFrame()\n",
    "final_df = pd.concat([preds_A, preds_B, preds_C], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"/Users/mayureshnene/Desktop/Mayuresh/Offense_Marathi/Experiments/Data_annotated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
