{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the local git repo for Indic NLP library\n",
    "INDIC_NLP_LIB_HOME=r\"/Users/mayuresh/Desktop/indic_nlp_library\"\n",
    "\n",
    "# The path to the local git repo for Indic NLP Resources\n",
    "INDIC_NLP_RESOURCES=r\"/Users/mayuresh/Desktop/indic_nlp_resources\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
    "from indicnlp import common\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
    "from indicnlp import loader\n",
    "loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       GRP\n",
       "1       GRP\n",
       "2       GRP\n",
       "3       GRP\n",
       "4       GRP\n",
       "       ... \n",
       "2493    OTH\n",
       "2494    OTH\n",
       "2495    OTH\n",
       "2496    OTH\n",
       "2497    OTH\n",
       "Name: subtask_c, Length: 2498, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = pd.read_csv(\"/Users/mayuresh/Desktop/MOLD/Mold/MOLD_Training2.csv\")\n",
    "training_dataset.head()\n",
    "training_dataset.dropna()\n",
    "training_dataset['subtask_c'].fillna(\"NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ‡§∞‡§æ‡§Æ ‡§ï‡§¶‡§Æ ‡§µ‡§æ‡§ó‡§£‡•ç‡§Ø‡§æ‡§§ ‡§®‡§æ‡§π‡•Ä ‡§§‡§∞ ‡§¨‡•ã‡§≤‡§£‡•ç‡§Ø‡§æ‡§§ ‡§ö‡•Å‡§ï‡§≤‡§æ  ‡§¨‡•Ä‡§ú‡•á‡§™...\n",
       "1       ‡§π‡•Ä‡§ö ‡§ï‡§æ ‡§§‡•Å‡§Æ‡§ö‡•Ä ‡§∂‡§ø‡§µ‡§∏‡•á‡§®‡•á ‡§ö‡§ø ‡§∂‡§ø‡§ï‡§µ‡§£... ‡§Ü‡§™‡§≤‡•Ä ‡§Ü‡§à ‡§Æ‡•ç‡§π‡§£‡§ú...\n",
       "2       ‡§π‡•á ‡§µ‡§æ‡§ö‡§æ ‡§ó‡§æ‡§¢‡§µ‡§æ‡§Ç‡§®‡•ã. ‡§Ü‡§£‡§ø ‡§π‡•á ‡§π‡•Ä ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§ï‡•Ä ‡§§‡•Å‡§Æ‡§ö‡•ç‡§Ø‡§æ ‡§Æ...\n",
       "3              ‡§≠‡§ï‡•ç‡§§ ‡§Ü‡§Ç‡§ß‡§≥‡•á ‡§Ö‡§∏‡§§‡§æ‡§§. ‡§Æ‡•Ç‡§∞‡•ç‡§ñ‡§æ ‡§®‡§æ ‡§ï‡§æ‡§π‡•Ä ‡§ï‡§≥‡§§ ‡§®‡§æ‡§π‡•Ä.\n",
       "4       ‡•ß‡•Ø‡•¨‡•¨ ‡§∏‡§æ‡§≤‡•Ä ‡§õ‡§§‡•ç‡§∞‡§™‡§§‡•Ä ‡§∂‡§ø‡§µ‡§æ‡§ú‡•Ä ‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú ‡§Ø‡§æ ‡§ú‡§æ‡§ó‡§§‡§ø‡§ï ‡§¶‡§∞‡•ç‡§ú...\n",
       "                              ...                        \n",
       "2493     ‡§™‡§£ ‡§¨‡§∞‡•á‡§ö ‡§ú‡§£ ‡§ú‡§®‡•ç‡§Æ‡§ú‡§æ‡§§ ‡§Æ‡§Ç‡§¶   ‡§Æ‡•Ç‡§∞‡•ç‡§ñ ‡§Ü‡§£‡§ø ‡§Ö‡§Ç‡§ß‡§≠‡§ï‡•ç‡§§ ‡§Ö‡§∏...\n",
       "2494     ‡§ï‡§∞‡•ç‡§®‡§æ‡§ü‡§ï ‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§ú‡•á‡§µ‡•ç‡§π‡§æ ‡§õ‡§§‡•ç‡§∞‡§™‡§§‡•Ä ‡§∂‡§ø‡§µ‡§æ‡§ú‡•Ä ‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú‡§æ‡§Ç‡§ö...\n",
       "2495     ‡§Æ‡§Ç‡§¶...‡§∏‡§§‡•ç‡§§‡•á‡§ö‡•á ‡§≤‡§æ‡§≤‡§ö‡•Ä ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§Æ‡•ç‡§π‡§£‡•Ç‡§® ‡§Ü‡§ú ‡§ó‡•Å‡§Ç‡§°‡§æ‡§Ç‡§®‡§æ...\n",
       "2496                           ‡§Ü‡§Ø‡§ü‡•Ä ‡§∏‡•á‡§≤ ‡§µ‡§æ‡§≤‡•á ‡§¶‡§≥‡§≠‡§¶‡•ç‡§∞‡•Ä ü§£ü§£ü§£ \n",
       "2497     ‡§Æ‡§µ‡§ø‡§Ü ‡§ö‡•á ‡§ö‡§æ‡§ü‡•á ‡§ï‡§æ‡§Ø ‡§ó‡•ã‡§ü‡•ç‡§Ø‡§æ ‡§ö‡•ã‡§≥‡§§ ‡§¨‡§∏‡§≤‡•á ‡§ï‡§æ‡§Ø ‡§®‡•á‡§§‡•ç‡§Ø‡§æ‡§Ç...\n",
       "Name: tweet, Length: 2498, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = training_dataset[\"tweet\"]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_A = training_dataset[[\"subtask_a\"]]\n",
    "level_B = training_dataset.query(\"subtask_a == 'Offensive'\")[[\"subtask_b\"]]\n",
    "level_C = training_dataset.query(\"subtask_b == 'TIN'\")[[\"subtask_c\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import indic_tokenize  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡§Ö‡§ß‡§ø‡§ï ', '‡§Ö‡§®‡•á‡§ï ', '‡§Ö‡§∂‡•Ä ', '‡§Ö‡§∏‡§≤‡§Ø‡§æ‡§ö‡•á', '‡§Ö‡§∏‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ', '‡§Ö‡§∏‡§æ', '‡§Ö‡§∏‡•Ç‡§®', '‡§Ö‡§∏‡•á', '‡§Ü‡§ú', '‡§Ü‡§£‡§ø', '‡§Ü‡§§‡§æ', '‡§Ü‡§™‡§≤‡•ç‡§Ø‡§æ', '‡§Ü‡§≤‡§æ', '‡§Ü‡§≤‡•Ä', '‡§Ü‡§≤‡•á', '‡§Ü‡§π‡•á', '‡§Ü‡§π‡•á‡§§', '‡§è‡§ï', '‡§è‡§ï‡§æ', '‡§ï‡§Æ‡•Ä', '‡§ï‡§∞‡§£‡§Ø‡§æ‡§§', '‡§ï‡§∞‡•Ç‡§®', '‡§ï‡§æ', '‡§ï‡§æ‡§Æ', '‡§ï‡§æ‡§Ø', '‡§ï‡§æ‡§π‡•Ä', '‡§ï‡§ø‡§µ‡§æ', '‡§ï‡•Ä', '‡§ï‡•á‡§≤‡§æ', '‡§ï‡•á‡§≤‡•Ä', '‡§ï‡•á‡§≤‡•á', '‡§ï‡•ã‡§ü‡•Ä', '‡§ó‡•á‡§≤‡•ç‡§Ø‡§æ', '‡§ò‡•á‡§ä‡§®', '‡§ú‡§æ‡§§', '‡§ù‡§æ‡§≤‡§æ', '‡§ù‡§æ‡§≤‡•Ä', '‡§ù‡§æ‡§≤‡•á', '‡§ù‡§æ‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ', '‡§ü‡§æ', '‡§°‡•â', '‡§§‡§∞', '‡§§‡§∞‡•Ä', '‡§§‡§∏‡•á‡§ö', '‡§§‡§æ', '‡§§‡•Ä', '‡§§‡•Ä‡§®', '‡§§‡•á', '‡§§‡•ã', '‡§§‡•ç‡§Ø‡§æ', '‡§§‡•ç‡§Ø‡§æ‡§ö‡§æ', '‡§§‡•ç‡§Ø‡§æ‡§ö‡•Ä', '‡§§‡•ç‡§Ø‡§æ‡§ö‡•ç‡§Ø‡§æ', '‡§§‡•ç‡§Ø‡§æ‡§®‡§æ', '‡§§‡•ç‡§Ø‡§æ‡§®‡•Ä', '‡§§‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á', '‡§§‡•ç‡§∞‡•Ä', '‡§¶‡§ø‡§≤‡•Ä', '‡§¶‡•ã‡§®', '‡§®', '‡§®‡§æ‡§π‡•Ä', '‡§®‡§ø‡§∞‡•ç‡§£‡•ç‡§Ø', '‡§™‡§£', '‡§™‡§Æ', '‡§™‡§∞‡§Ø‡§§‡§®', '‡§™‡§æ‡§ü‡•Ä‡§≤', '‡§Æ', '‡§Æ‡§æ‡§§‡•ç‡§∞', '‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä', '‡§Æ‡•Ä', '‡§Æ‡•Å‡§¨‡•Ä', '‡§Æ‡•ç‡§π‡§£‡§ú‡•á', '‡§Æ‡•ç‡§π‡§£‡§æ‡§≤‡•á', '‡§Æ‡•ç‡§π‡§£‡•Ç‡§®', '‡§Ø‡§æ', '‡§Ø‡§æ‡§ö‡§æ', '‡§Ø‡§æ‡§ö‡•Ä', '‡§Ø‡§æ‡§ö‡•ç‡§Ø‡§æ', '‡§Ø‡§æ‡§®‡§æ', '‡§Ø‡§æ‡§®‡•Ä', '‡§Ø‡•á‡§£‡§æ‡§∞', '‡§Ø‡•á‡§§', '‡§Ø‡•á‡§•‡•Ä‡§≤', '‡§Ø‡•á‡§•‡•á', '‡§≤‡§æ‡§ñ', '‡§µ', '‡§µ‡•ç‡§Ø‡§ï‡§§', '‡§∏‡§∞‡•ç‡§µ', '‡§∏‡§æ‡§ó‡§ø‡§§‡•ç‡§≤‡•á', '‡§∏‡•Å‡§∞‡•Ç', '‡§π‡§ú‡§æ‡§∞', '‡§π‡§æ', '‡§π‡•Ä', '‡§π‡•á', '‡§π‡•ã‡§£‡§æ‡§∞', '‡§π‡•ã‡§§', '‡§π‡•ã‡§§‡§æ', '‡§π‡•ã‡§§‡•Ä', '‡§π‡•ã‡§§‡•á']\n"
     ]
    }
   ],
   "source": [
    "stopwords_file = open(\"/Users/mayuresh/Desktop/MOLD/Mold/stopwords.txt\")\n",
    "stopwords = stopwords_file.read().splitlines()\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tweet):\n",
    "    removal_list = ['URL','\\'ve','n\\'t','\\'s','\\'m','!']\n",
    "    for element in removal_list:\n",
    "        tweet = str(tweet).replace(element,'')\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tweet in tweets:\n",
    "#     tweet = remove_noise(str(tweet))\n",
    "iterator_map = map(clean,tweets)\n",
    "tweets = list(iterator_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import indic_tokenize  \n",
    "import copy\n",
    "collective_tweets = copy.deepcopy(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.morph import unsupervised_morph \n",
    "from indicnlp import common\n",
    "\n",
    "analyzer=unsupervised_morph.UnsupervisedMorphAnalyzer('mr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    return indic_tokenize.trivial_tokenize(tweet)\n",
    "\n",
    "def morph(tweet):\n",
    "    analyzed_tokens=analyzer.morph_analyze_document(str(tweet).split(' '))\n",
    "    return analyzed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "Tokenize..: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2498/2498 [00:01<00:00, 2392.54it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"Tokenize..\")\n",
    "#all_tweets[\"tokens\"] = all_tweets['tweet'].progress_apply(tokenize)\n",
    "collective_tweets[\"tokens\"] = collective_tweets['tweet'].progress_apply(morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = collective_tweets[\"tokens\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfid_vectorizer(vector):\n",
    "\t## Creates and stores an instance of the TfidfVectorizer class. This will be used further to extract our data as tf-idf features.\n",
    "\tvectorizer = TfidfVectorizer()\n",
    "\tuntokenized_data =[' '.join(tweet) for tweet in tqdm(vector, \"Vectorizing...\")]\n",
    "\tvectorizer = vectorizer.fit(untokenized_data)\n",
    "\tvectors = vectorizer.transform(untokenized_data).toarray()\n",
    "\treturn vectors\n",
    "\n",
    "def get_vectors(vectors, labels, keyword):\n",
    "\t'''\n",
    "\tReturns a matrix for vectors. Zips vectors and labels IF and only if length of vector list is the same as length of the labels list. \n",
    "\tElse, the function gets terminated.\n",
    "\t@param vectors These are the vectors for a given label.\n",
    "\t@param labels These are the label values for the given label.\n",
    "\t@param keyword which is the label to annotate for.\n",
    "\t'''\n",
    "\tif len(vectors) != len(labels):\n",
    "\t\tprint(\"Unmatching sizes!\")\n",
    "\t\treturn\n",
    "\t\n",
    "\t## Stores a new list to append the zipped vectors and labels into.\n",
    "\tresult = list()\n",
    "\tfor vector, label in zip(vectors, labels):\n",
    "\t\tif label == keyword:\n",
    "\t\t\tresult.append(vector)\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2498/2498 [00:00<00:00, 344639.04it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors_level_A = tfid_vectorizer(vector)\n",
    "labels_level_a = level_A['subtask_a'].values.tolist()\n",
    "vectors_level_B = get_vectors(vectors_level_A, labels_level_a, \"Offensive\") \n",
    "\n",
    "labels_level_b = level_B['subtask_b'].values.tolist() \n",
    "\n",
    "## Numerical Vectors C\n",
    "vectors_level_c = get_vectors(vectors_level_B, labels_level_b, \"TIN\") \n",
    "\n",
    "##Subtask C Labels\n",
    "labels_level_c = level_C['subtask_c'].values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
