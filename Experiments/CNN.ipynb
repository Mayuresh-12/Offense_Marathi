{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77a5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import Sequential\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9012348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0       Offensive\n",
       "1       Offensive\n",
       "2       Offensive\n",
       "3       Offensive\n",
       "4       Offensive\n",
       "          ...    \n",
       "3484    Offensive\n",
       "3485    Offensive\n",
       "3486    Offensive\n",
       "3487    Offensive\n",
       "3488    Offensive\n",
       "Name: subtask_a, Length: 3489, dtype: object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = pd.read_csv(\"/Users/mayureshnene/Desktop/Mayuresh/Offense_Marathi/Marathi_Train.csv\")\n",
    "test_data = pd.read_csv(\"/Users/mayureshnene/Desktop/Mayuresh/Offense_Marathi/Marathi_Test.csv\")\n",
    "training_dataset.head()\n",
    "training_dataset['subtask_a'].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b839ddac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/keras_preprocessing/text.py:180: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=2500, lower=True,split=' ')\n",
    "tokenizer.fit_on_texts(training_dataset['Tweet'].values)\n",
    "X = tokenizer.texts_to_sequences(training_dataset['Tweet'].values)\n",
    "X = pad_sequences(X)\n",
    "Y = pd.get_dummies(training_dataset['subtask_a']).values\n",
    "\n",
    "X_valid = tokenizer.texts_to_sequences(test_data['Tweet'].values)\n",
    "X_valid = pad_sequences(X_valid)\n",
    "Y_valid = pd.get_dummies(test_data['subtask_a']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6c064e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='embedding_2_input'), name='embedding_2_input', description=\"created by layer 'embedding_2_input'\"), but it was called on an input with incompatible shape (None, 62).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='embedding_2_input'), name='embedding_2_input', description=\"created by layer 'embedding_2_input'\"), but it was called on an input with incompatible shape (None, 62).\n",
      "137/140 [============================>.] - ETA: 0s - loss: 0.6627 - accuracy: 0.6627WARNING:tensorflow:Model was constructed with shape (None, 100) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='embedding_2_input'), name='embedding_2_input', description=\"created by layer 'embedding_2_input'\"), but it was called on an input with incompatible shape (None, 28).\n",
      "140/140 [==============================] - 2s 12ms/step - loss: 0.6624 - accuracy: 0.6627 - val_loss: 0.7159 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.6427 - accuracy: 0.6667 - val_loss: 0.7338 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.6405 - accuracy: 0.6667 - val_loss: 0.7403 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.6402 - accuracy: 0.6667 - val_loss: 0.7430 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.6396 - accuracy: 0.6667 - val_loss: 0.7440 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.6395 - accuracy: 0.6667 - val_loss: 0.7448 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.6390 - accuracy: 0.6667 - val_loss: 0.7451 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.6388 - accuracy: 0.6667 - val_loss: 0.7454 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.6387 - accuracy: 0.6667 - val_loss: 0.7459 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.6384 - accuracy: 0.6667 - val_loss: 0.7461 - val_accuracy: 0.5000\n",
      "26/26 - 0s - loss: 0.7461 - accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.convolutional import Conv1D\n",
    "from keras import layers\n",
    "embedding_dim = 128\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 100\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.add(layers.Flatten())\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X, Y, \n",
    "          epochs=10, \n",
    "          validation_data=(X_valid, Y_valid),\n",
    "          batch_size=25)\n",
    "\n",
    "score, acc = model.evaluate(X_valid, Y_valid, verbose = 2, batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab384a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
